{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据（取最小行数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878 4\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "min_rows = float('inf') \n",
    "\n",
    "for filename in os.listdir(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\"):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\", filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.drop(columns=[\"Date\"])\n",
    "        datalist.append(df.values)\n",
    "        min_rows = min(min_rows, df.shape[0])      \n",
    "\n",
    "data_list = [data[:min_rows] for data in datalist]\n",
    "\n",
    "print(min_rows,len(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理最后一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21791101  0.221791    0.214866    0.21748801  0.21748801 -2.28482287]\n",
      " [ 0.218256    0.21906801  0.20525999  0.20648301  0.20648301 -2.33154956]\n",
      " [ 0.205948    0.21445601  0.205459    0.21043     0.21043    -2.37803102]\n",
      " ...\n",
      " [ 0.36812201  0.368543    0.35613599  0.35653099  0.35653099 -0.58243597]\n",
      " [ 0.35652399  0.357225    0.338911    0.34284601  0.34284601 -0.51650078]\n",
      " [ 0.34283099  0.34531301  0.33533299  0.34501699  0.34501699 -0.80060086]]\n"
     ]
    }
   ],
   "source": [
    "def logVolume(X: np.ndarray):\n",
    "    volume = X[:, -1]  \n",
    "    volume_log = np.log(volume + 1)  \n",
    "\n",
    "    mean_log = np.mean(volume_log)  \n",
    "    std_log = np.std(volume_log)  \n",
    "\n",
    "    volume_std = (volume_log - mean_log) / std_log  \n",
    "\n",
    "    X[:, -1] = volume_std  \n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "for data in data_list:\n",
    "    data=logVolume(data)    \n",
    "\n",
    "print(data_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建标签函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "days=10\n",
    "\n",
    "ptsl = [0.05, 0.05]  \n",
    "return_min = 0.005\n",
    "def triple_barrier(close: np.ndarray, days=10, pts=[0.05, 0.05] ):\n",
    "    bin = np.zeros(close.size, dtype=int)  \n",
    "    for i in range(close.size):\n",
    "        for d in range(days):\n",
    "            index = min(i + d + 1, close.size - 1)  \n",
    "            if close[index] >= close[i] * (1 + ptsl[0]):  \n",
    "                bin[i] = 1\n",
    "                break\n",
    "            elif close[index] <= close[i] * (1 - ptsl[1]):  \n",
    "                bin[i] = -1\n",
    "                break\n",
    "    \n",
    "    return bin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1878,)\n"
     ]
    }
   ],
   "source": [
    "binmat=np.full((min_rows, 4), 3)\n",
    "for i in range(4):\n",
    "    binmat[:,i]=triple_barrier(data_list[i][:,3])\n",
    "\n",
    "label=np.mean(binmat, axis=1)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1878, 6)\n",
      "(1878, 24)\n"
     ]
    }
   ],
   "source": [
    "# 确保所有二维数组具有相同的形状\n",
    "shapes = {data.shape for data in data_list}\n",
    "if len(shapes) > 1:\n",
    "    raise ValueError(\"所有 CSV 文件中的数据形状必须一致\")\n",
    "    \n",
    "dataset=np.array(data_list)\n",
    "print(dataset.shape)\n",
    "dataset = dataset.transpose(1, 0, 2).reshape(1878, -1)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary model with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=(label + 1)/2\n",
    "targets=np.where(targets > 0.5, 1, 0)\n",
    "indices = np.arange(len(dataset))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(dataset,targets,indices,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asset_exposure_constraint(weights, max_exposure=0.5):\n",
    "    \"\"\"Ensure no asset weight exceeds max_exposure.\"\"\"\n",
    "    return all(abs(w) <= max_exposure for w in weights)\n",
    "\n",
    "def sector_diversification_constraint(weights, sector_mapping, min_sectors=2):\n",
    "    \"\"\"Ensure allocation spans at least min_sectors.\"\"\"\n",
    "    unique_sectors = set(sector_mapping[np.nonzero(weights)])\n",
    "    return len(unique_sectors) >= min_sectors\n",
    "\n",
    "\n",
    "def risk_tolerance_constraint(weights, cov_matrix, max_risk=0.05):\n",
    "    \"\"\"Ensure portfolio variance is within max_risk.\"\"\"\n",
    "    portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    #print(portfolio_variance)\n",
    "    return portfolio_variance <= max_risk\n",
    "\n",
    "def apply_constraints(weights, sector_mapping, cov_matrix):\n",
    "    \"\"\"Check all constraints.\"\"\"\n",
    "    return (\n",
    "        asset_exposure_constraint(weights, max_exposure=0.5) and\n",
    "        sector_diversification_constraint(weights, sector_mapping, min_sectors=2) and\n",
    "        risk_tolerance_constraint(weights, cov_matrix, max_risk=0.05)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary model using XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_primary = model.predict(X_test)\n",
    "primary_signals = np.where(y_pred_primary > 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 1 0 1 0]\n",
      "376\n",
      "(376, 24)\n"
     ]
    }
   ],
   "source": [
    "print(primary_signals)\n",
    "print(len(primary_signals))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1502, 24)\n",
      "(376, 24)\n",
      "(376,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 4)\n",
      "[[0.00975429 0.00216908 0.00084753 0.01012304]\n",
      " [0.00216908 0.06281248 0.01614855 0.00204031]\n",
      " [0.00084753 0.01614855 0.01446641 0.00049159]\n",
      " [0.01012304 0.00204031 0.00049159 0.02662378]]\n"
     ]
    }
   ],
   "source": [
    "# Assume asset returns, sector mapping, and covariance matrix as placeholders\n",
    "sector_mapping = np.array([1,2,3,4])\n",
    "\n",
    "Closelist=[]\n",
    "for filename in os.listdir(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\"):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\", filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[\"Adj Close\"]\n",
    "        Closelist.append(df.values)\n",
    "        min_rows = min(min_rows, df.shape[0])\n",
    "Closelist = [data[:min_rows] for data in Closelist]\n",
    "Close=np.array(Closelist).T\n",
    "Close_test=Close[sorted(indices_test),:]\n",
    "print(Close_test.shape)\n",
    "returns = (Close_test[1:] - Close_test[:-1]) / Close_test[:-1]  \n",
    "cov_matrix = np.cov(returns, rowvar=False)\n",
    "print(cov_matrix)\n",
    "\n",
    "asset_returns = np.random.normal(0.01, 0.02, size=dataset.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary model (meta-labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Develop secondary model (meta-labeling) 二级模型，即meta-labeling\n",
    "# Create meta-labels for secondary model training\n",
    "meta_labels = primary_signals == y_test\n",
    "t_indices = np.arange(len(X_test))\n",
    "X_train_meta, X_test_meta, y_train_meta, y_test_meta, t_indices_train, t_indices_test = train_test_split(\n",
    "    X_test, meta_labels, t_indices, test_size=0.5, random_state=42\n",
    ")\n",
    "X_test_meta = pd.DataFrame(X_test_meta)\n",
    "\n",
    "\n",
    "# Train and test secondary model (meta-labeling) 训练模型并生成二级信号\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train_meta, y_train_meta)\n",
    "y_pred_secondary = logistic.predict(X_test_meta)\n",
    "\n",
    "\n",
    "# Step 4: Filtering, denoising, and refining signals 进行信号过滤，生成最终信号\n",
    "refined_signals = primary_signals.copy()\n",
    "zero_pred_mask = (y_pred_secondary == 0)\n",
    "meta_zero_indices = np.where(zero_pred_mask)[0]  \n",
    "original_test_indices = t_indices_test[meta_zero_indices]  \n",
    "#print(original_test_indices)\n",
    "for i in original_test_indices:\n",
    "    refined_signals[i] = (1-refined_signals[i])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376 12\n",
      "(376,) [0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(refined_signals),np.sum(abs(refined_signals-primary_signals)))\n",
    "print(refined_signals.shape,refined_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate modified(final) portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample portfolio based on signals\n",
    "def construct_portfolio(signals, asset_returns, sector_mapping, cov_matrix):\n",
    "    \"\"\"Construct a portfolio based on signals and constraints.\"\"\"\n",
    "    weights = np.zeros((len(signals),4))\n",
    "    for i, signal in enumerate(signals):\n",
    "        if signal == 1:\n",
    "            # Placeholder: Assign random initial weights to activated signals\n",
    "            weights[i,:] = np.random.rand(4)\n",
    "            # Normalize weights\n",
    "            weights[i,:] /= np.sum(weights[i,:])\n",
    "            # Apply constraints\n",
    "            if apply_constraints(weights[i,:], sector_mapping, cov_matrix):\n",
    "                weights[i,:] = weights[i,:]\n",
    "                #print(\"Constraints good.\")\n",
    "            else:\n",
    "                #print(weights[i,:])\n",
    "                weights[i,:] = np.zeros_like(weights[i,:]) #adjust weights\n",
    "                #print(\"Constraints not satisfied. Adjust weights.\")\n",
    "\n",
    "    return weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Portfolio Weights:\n",
      " [[0.         0.         0.         0.        ]\n",
      " [0.34400156 0.18495151 0.38604635 0.08500058]\n",
      " [0.         0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20318597 0.14334141 0.34163102 0.3118416 ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "active trade days: 102 (376, 4)\n"
     ]
    }
   ],
   "source": [
    "final_portfolio_weights = construct_portfolio(refined_signals, asset_returns, sector_mapping, cov_matrix)\n",
    "tradedays=0\n",
    "# Output portfolio weights\n",
    "print(\"Final Portfolio Weights:\\n\", final_portfolio_weights)\n",
    "for i in range(final_portfolio_weights.shape[0]):\n",
    "    if np.sum(final_portfolio_weights[i]) !=0:\n",
    "        tradedays+=1\n",
    "print(\"active trade days:\",tradedays,final_portfolio_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/stock1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 57\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     51\u001b[0m     stock_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/stock1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/stock2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/stock3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/stock4.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     56\u001b[0m     ]\n\u001b[0;32m---> 57\u001b[0m     sharpe \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_sharpe_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m年化夏普比率: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msharpe\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m, in \u001b[0;36mcalculate_sharpe_ratio\u001b[0;34m(stock_files)\u001b[0m\n\u001b[1;32m      4\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stock_files, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 规范列名：假设原始列名为 open, close, high, low, weight, date\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 确保日期列名统一\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     })\n",
      "File \u001b[0;32m~/anaconda3/envs/ywc/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ywc/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/ywc/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ywc/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/ywc/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/stock1.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_sharpe_ratio(stock_files):\n",
    "\n",
    "    # 读取并合并数据\n",
    "    dfs = []\n",
    "    for i, file in enumerate(stock_files, 1):\n",
    "        df = pd.read_csv(file)\n",
    "        # 规范列名：假设原始列名为 open, close, high, low, weight, date\n",
    "        df = df.rename(columns={\n",
    "            'open': f'stock{i}_open',\n",
    "            'close': f'stock{i}_close',\n",
    "            'high': f'stock{i}_high',\n",
    "            'low': f'stock{i}_low',\n",
    "            'weight': f'weight{i}',\n",
    "            'date': 'date'  # 确保日期列名统一\n",
    "        })\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # 按日期合并所有数据（假设日期已对齐）\n",
    "    merged = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged = merged.merge(df, on='date', how='inner')\n",
    "    \n",
    "    # 计算每个股票的日收益率\n",
    "    for i in range(1, 5):\n",
    "        open_col = f'stock{i}_open'\n",
    "        close_col = f'stock{i}_close'\n",
    "        merged[f'return{i}'] = (merged[close_col] - merged[open_col]) / merged[open_col]\n",
    "    \n",
    "    # 计算组合日收益率（包含现金部分）\n",
    "    weights = merged[[f'weight{i}' for i in range(1, 5)]]\n",
    "    returns = merged[[f'return{i}' for i in range(1, 5)]]\n",
    "    merged['portfolio_return'] = (weights * returns).sum(axis=1)\n",
    "    \n",
    "    # 现金部分权重（总和≤1）\n",
    "    cash_weight = 1 - weights.sum(axis=1)\n",
    "    merged['portfolio_return'] += cash_weight * 0  # 明确体现现金部分\n",
    "    \n",
    "    # 计算夏普比率\n",
    "    mu_daily = merged['portfolio_return'].mean()\n",
    "    sigma_daily = merged['portfolio_return'].std()\n",
    "    \n",
    "    if sigma_daily == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 年化（按365天）\n",
    "    sharpe_ratio = (mu_daily * 365) / (sigma_daily * np.sqrt(365))\n",
    "    return sharpe_ratio\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    stock_files = [\n",
    "        'path/to/stock1.csv',\n",
    "        'path/to/stock2.csv',\n",
    "        'path/to/stock3.csv',\n",
    "        'path/to/stock4.csv'\n",
    "    ]\n",
    "    sharpe = calculate_sharpe_ratio(stock_files)\n",
    "    print(f\"年化夏普比率: {sharpe:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ywc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
