{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据（取最小行数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878 4\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "min_rows = float('inf') \n",
    "\n",
    "for filename in os.listdir(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\"):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\", filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.drop(columns=[\"Date\"])\n",
    "        datalist.append(df.values)\n",
    "        min_rows = min(min_rows, df.shape[0])      \n",
    "\n",
    "data_list = [data[:min_rows] for data in datalist]\n",
    "\n",
    "print(min_rows,len(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理最后一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21791101  0.221791    0.214866    0.21748801  0.21748801 -2.28482287]\n",
      " [ 0.218256    0.21906801  0.20525999  0.20648301  0.20648301 -2.33154956]\n",
      " [ 0.205948    0.21445601  0.205459    0.21043     0.21043    -2.37803102]\n",
      " ...\n",
      " [ 0.36812201  0.368543    0.35613599  0.35653099  0.35653099 -0.58243597]\n",
      " [ 0.35652399  0.357225    0.338911    0.34284601  0.34284601 -0.51650078]\n",
      " [ 0.34283099  0.34531301  0.33533299  0.34501699  0.34501699 -0.80060086]]\n"
     ]
    }
   ],
   "source": [
    "def logVolume(X: np.ndarray):\n",
    "    volume = X[:, -1]  \n",
    "    volume_log = np.log(volume + 1)  \n",
    "\n",
    "    mean_log = np.mean(volume_log)  \n",
    "    std_log = np.std(volume_log)  \n",
    "\n",
    "    volume_std = (volume_log - mean_log) / std_log  \n",
    "\n",
    "    X[:, -1] = volume_std  \n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "for data in data_list:\n",
    "    data=logVolume(data)    \n",
    "\n",
    "print(data_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建标签函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "days=10\n",
    "\n",
    "ptsl = [0.05, 0.05]  \n",
    "return_min = 0.005\n",
    "def triple_barrier(close: np.ndarray, days=10, pts=[0.05, 0.05] ):\n",
    "    bin = np.zeros(close.size, dtype=int)  \n",
    "    for i in range(close.size):\n",
    "        for d in range(days):\n",
    "            index = min(i + d + 1, close.size - 1)  \n",
    "            if close[index] >= close[i] * (1 + ptsl[0]):  \n",
    "                bin[i] = 1\n",
    "                break\n",
    "            elif close[index] <= close[i] * (1 - ptsl[1]):  \n",
    "                bin[i] = -1\n",
    "                break\n",
    "    \n",
    "    return bin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1878,)\n"
     ]
    }
   ],
   "source": [
    "binmat=np.full((min_rows, 4), 3)\n",
    "for i in range(4):\n",
    "    binmat[:,i]=triple_barrier(data_list[i][:,3])\n",
    "\n",
    "label=np.mean(binmat, axis=1)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1878, 6)\n",
      "(1878, 24)\n"
     ]
    }
   ],
   "source": [
    "# 确保所有二维数组具有相同的形状\n",
    "shapes = {data.shape for data in data_list}\n",
    "if len(shapes) > 1:\n",
    "    raise ValueError(\"所有 CSV 文件中的数据形状必须一致\")\n",
    "    \n",
    "dataset=np.array(data_list)\n",
    "print(dataset.shape)\n",
    "dataset = dataset.transpose(1, 0, 2).reshape(1878, -1)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary model with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=(label + 1)/2\n",
    "targets=np.where(targets > 0.5, 1, 0)\n",
    "indices = np.arange(len(dataset))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(dataset,targets,indices,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asset_exposure_constraint(weights, max_exposure=0.5):\n",
    "    \"\"\"Ensure no asset weight exceeds max_exposure.\"\"\"\n",
    "    return all(abs(w) <= max_exposure for w in weights)\n",
    "\n",
    "def sector_diversification_constraint(weights, sector_mapping, min_sectors=2):\n",
    "    \"\"\"Ensure allocation spans at least min_sectors.\"\"\"\n",
    "    unique_sectors = set(sector_mapping[np.nonzero(weights)])\n",
    "    return len(unique_sectors) >= min_sectors\n",
    "\n",
    "\n",
    "def risk_tolerance_constraint(weights, cov_matrix, max_risk=0.05):\n",
    "    \"\"\"Ensure portfolio variance is within max_risk.\"\"\"\n",
    "    portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    #print(portfolio_variance)\n",
    "    return portfolio_variance <= max_risk\n",
    "\n",
    "def apply_constraints(weights, sector_mapping, cov_matrix):\n",
    "    \"\"\"Check all constraints.\"\"\"\n",
    "    return (\n",
    "        asset_exposure_constraint(weights, max_exposure=0.5) and\n",
    "        sector_diversification_constraint(weights, sector_mapping, min_sectors=2) and\n",
    "        risk_tolerance_constraint(weights, cov_matrix, max_risk=0.05)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary model using XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_primary = model.predict(X_test)\n",
    "primary_signals = np.where(y_pred_primary > 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 1 0 1 0]\n",
      "376\n",
      "(376, 24)\n"
     ]
    }
   ],
   "source": [
    "print(primary_signals)\n",
    "print(len(primary_signals))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1502, 24)\n",
      "(376, 24)\n",
      "(376,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 4)\n",
      "[[0.00975429 0.00216908 0.00084753 0.01012304]\n",
      " [0.00216908 0.06281248 0.01614855 0.00204031]\n",
      " [0.00084753 0.01614855 0.01446641 0.00049159]\n",
      " [0.01012304 0.00204031 0.00049159 0.02662378]]\n"
     ]
    }
   ],
   "source": [
    "# Assume asset returns, sector mapping, and covariance matrix as placeholders\n",
    "sector_mapping = np.array([1,2,3,4])\n",
    "\n",
    "Closelist=[]\n",
    "for filename in os.listdir(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\"):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(\"/home/yichuan/ywc/meta-labeling/cryptocurrency\", filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[\"Adj Close\"]\n",
    "        Closelist.append(df.values)\n",
    "        min_rows = min(min_rows, df.shape[0])\n",
    "Closelist = [data[:min_rows] for data in Closelist]\n",
    "Close=np.array(Closelist).T\n",
    "Close_test=Close[sorted(indices_test),:]\n",
    "print(Close_test.shape)\n",
    "returns = (Close_test[1:] - Close_test[:-1]) / Close_test[:-1]  \n",
    "cov_matrix = np.cov(returns, rowvar=False)\n",
    "print(cov_matrix)\n",
    "\n",
    "asset_returns = np.random.normal(0.01, 0.02, size=dataset.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary model (meta-labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Develop secondary model (meta-labeling) 二级模型，即meta-labeling\n",
    "# Create meta-labels for secondary model training\n",
    "meta_labels = primary_signals == y_test\n",
    "t_indices = np.arange(len(X_test))\n",
    "X_train_meta, X_test_meta, y_train_meta, y_test_meta, t_indices_train, t_indices_test = train_test_split(\n",
    "    X_test, meta_labels, t_indices, test_size=0.5, random_state=42\n",
    ")\n",
    "X_test_meta = pd.DataFrame(X_test_meta)\n",
    "\n",
    "\n",
    "# Train and test secondary model (meta-labeling) 训练模型并生成二级信号\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train_meta, y_train_meta)\n",
    "y_pred_secondary = logistic.predict(X_test_meta)\n",
    "\n",
    "\n",
    "# Step 4: Filtering, denoising, and refining signals 进行信号过滤，生成最终信号\n",
    "refined_signals = primary_signals.copy()\n",
    "zero_pred_mask = (y_pred_secondary == 0)\n",
    "meta_zero_indices = np.where(zero_pred_mask)[0]  \n",
    "original_test_indices = t_indices_test[meta_zero_indices]  \n",
    "#print(original_test_indices)\n",
    "for i in original_test_indices:\n",
    "    refined_signals[i] = (1-refined_signals[i])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376 12\n",
      "(376,) [0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(refined_signals),np.sum(abs(refined_signals-primary_signals)))\n",
    "print(refined_signals.shape,refined_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate modified(final) portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample portfolio based on signals\n",
    "def construct_portfolio(signals, asset_returns, sector_mapping, cov_matrix):\n",
    "    \"\"\"Construct a portfolio based on signals and constraints.\"\"\"\n",
    "    weights = np.zeros((len(signals),4))\n",
    "    for i, signal in enumerate(signals):\n",
    "        if signal == 1:\n",
    "            # Placeholder: Assign random initial weights to activated signals\n",
    "            weights[i,:] = np.random.rand(4)\n",
    "            # Normalize weights\n",
    "            weights[i,:] /= np.sum(weights[i,:])\n",
    "            # Apply constraints\n",
    "            if apply_constraints(weights[i,:], sector_mapping, cov_matrix):\n",
    "                weights[i,:] = weights[i,:]\n",
    "                #print(\"Constraints good.\")\n",
    "            else:\n",
    "                #print(weights[i,:])\n",
    "                weights[i,:] = np.zeros_like(weights[i,:]) #adjust weights\n",
    "                #print(\"Constraints not satisfied. Adjust weights.\")\n",
    "\n",
    "    return weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Portfolio Weights:\n",
      " [[0.         0.         0.         0.        ]\n",
      " [0.34400156 0.18495151 0.38604635 0.08500058]\n",
      " [0.         0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20318597 0.14334141 0.34163102 0.3118416 ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "active trade days: 102 (376, 4)\n"
     ]
    }
   ],
   "source": [
    "final_portfolio_weights = construct_portfolio(refined_signals, asset_returns, sector_mapping, cov_matrix)\n",
    "tradedays=0\n",
    "# Output portfolio weights\n",
    "print(\"Final Portfolio Weights:\\n\", final_portfolio_weights)\n",
    "for i in range(final_portfolio_weights.shape[0]):\n",
    "    if np.sum(final_portfolio_weights[i]) !=0:\n",
    "        tradedays+=1\n",
    "print(\"active trade days:\",tradedays,final_portfolio_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the portfolio"
   ]
  }
